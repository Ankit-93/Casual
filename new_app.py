# -*- coding: utf-8 -*-
"""new_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g85xPNAvWeJ_4Yde-Lf-UTPxaRzMt9B_
"""

# !gdown https://drive.google.com/drive/u/0/folders/1fdIeWD1ijnNhUx2kC4MPDHGmRy8088P2 -O emotion_data.parquet
# !gdown --folder https://drive.google.com/drive/u/0/folders/1fdIeWD1ijnNhUx2kC4MPDHGmRy8088P2 -O /content/src
# !pip3 install datasets

import os, math
import numpy as np
import pandas as pd
from os import path
import tensorflow as tf
from datasets import load_dataset
from tokenizers import Tokenizer
from tensorflow.keras.utils import Sequence
from tokenizers.models import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace
from datasets import load_dataset, Dataset as HFDataset
from tensorflow.keras.layers import Embedding, Dropout, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.callbacks import TensorBoard
import traceback

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#from tensorflow.keras.preprocessing.text import Tokenizer
# !gdown https://drive.google.com/drive/u/0/folders/1fdIeWD1ijnNhUx2kC4MPDHGmRy8088P2 -O emotion_data.parquet
# !gdown --folder https://drive.google.com/drive/u/0/folders/1fdIeWD1ijnNhUx2kC4MPDHGmRy8088P2 -O /content/src

D_MODEL = 512 # Dimension of input token representation - embeddings
VOCAB_SIZE = 50000 # Number of tokens in vocabulary
MAX_SEQ_LEN = 32 # Maximum Sequence Length of Input and Output Sequence
NUM_LAYERS = 2 # Number of encoder and decoder stacks
ATTN_DROPOUT = 0.2 # Quantify the dropout of how much in attention
EPS = 1e-6 # A small value to avoid zero division error while normalization
FF_DROPOUT = 0.2 # How much dropout in FFN``
NUM_HEADS = 8 # Number of attention heads
RES_DROPOUT = 0.2 # Dropout for residual connection
TOKENIZER_PATH = "/content/tokenizer.json" # Tokenizer path
DATASET_ID = "/content/src/emotion_data.parquet" # Dataset path
SRC_CLN_NAME = "text" # Source column name
TGT_CLN_NAME = "label" # Target column name
LOG_DIR = "logs/v1" # Logging dir
BATCH_SIZE = 10 # Number of samples per batch
LR = 1e-4 # Learning Rate for optimizer
NUM_EPOCHS = 1 # Number of training epochs
MODEL_SAVE_PATH = "/content/classifier" # Model save path

class InputEmbeddings(tf.keras.layers.Layer):
    def __init__(self, d_model=D_MODEL, vocab_size=VOCAB_SIZE, seq_len=MAX_SEQ_LEN, dropout_p=FF_DROPOUT, **kwargs):
        super(InputEmbeddings, self).__init__(**kwargs)
        self.d_model = d_model
        self.vocab_size = vocab_size
        self.seq_len = seq_len
        self.dropout_p = dropout_p
        self.token_embedding = TokenEmbeddings(self.d_model, self.vocab_size)
        self.positional_encoding = PositionalEncoding(self.d_model, self.seq_len, self.dropout_p)

    def call(self, x):
        token_embed_x = self.token_embedding(x)
        inp_embedding = self.positional_encoding(token_embed_x)
        return inp_embedding

class MultiHeadAttention(tf.keras.layers.Layer):

    def __init__(self, d_model, num_heads: int, attn_dropout: float) -> None:
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.wq = Dense(d_model)  # Removed redundant arguments
        self.wk = Dense(d_model)  # Removed redundant arguments
        self.wv = Dense(d_model)  # Removed redundant arguments
        self.wo = Dense(d_model)
        self.dropout = Dropout(attn_dropout)

    def attention_calculation(self, query, key, value, mask):
        attention_scores = tf.matmul(query, key, transpose_b=True) / math.sqrt(self.head_dim)  # Replaced '@' with 'tf.matmul'
        if mask is not None:
            attention_scores = tf.where(mask == 0, tf.constant(-1e9, dtype=tf.float32), attention_scores)  # Used tf.where instead of masked_fill_
        attention_scores = tf.nn.softmax(attention_scores, axis=-1)  # Replaced '.softmax(dim=-1)' with 'tf.nn.softmax'
        if self.dropout is not None:
            attention_scores = self.dropout(attention_scores)
        return tf.matmul(attention_scores, value), attention_scores  # Replaced '@' with 'tf.matmul'

    def call(self, q, k, v, attn_mask):  # Replaced 'forward' with 'call'
        query = self.wq(q)
        key = self.wk(k)
        value = self.wv(v)
        query = tf.transpose(tf.reshape(query, [tf.shape(query)[0], -1, self.num_heads, self.head_dim]), [0, 2, 1, 3])  # Replaced '.view' with 'tf.reshape' and 'transpose' with 'tf.transpose'
        key = tf.transpose(tf.reshape(key, [tf.shape(key)[0], -1, self.num_heads, self.head_dim]), [0, 2, 1, 3])  # Replaced '.view' with 'tf.reshape' and 'transpose' with 'tf.transpose'
        value = tf.transpose(tf.reshape(value, [tf.shape(value)[0], -1, self.num_heads, self.head_dim]), [0, 2, 1, 3])  # Replaced '.view' with 'tf.reshape' and 'transpose' with 'tf.transpose'
        x, self.attention_scores = self.attention_calculation(query, key, value, attn_mask)
        x = tf.transpose(x, [0, 2, 1, 3])
        x = tf.reshape(x, [tf.shape(x)[0], -1, self.d_model])  # Replaced '.contiguous()' with 'tf.reshape'
        attn_out = self.wo(x)
        return attn_out

class TokenEmbeddings(tf.keras.layers.Layer):
    def __init__(self, d_model: int, vocab_size: int, **kwargs):
        super(TokenEmbeddings, self).__init__(**kwargs)
        self.d_model = d_model
        self.vocab_size = vocab_size
        self.embedding = Embedding(input_dim=vocab_size, output_dim=d_model)

    def call(self, x):
        token_embedding = self.embedding(x)
        return token_embedding

class PositionalEncoding(tf.keras.layers.Layer):
    def __init__(self, d_model: int, seq_len: int, dropout_p: float, **kwargs):
        super(PositionalEncoding, self).__init__(**kwargs)
        self.d_model = d_model
        self.seq_len = seq_len
        self.dropout_p = dropout_p
        position_encodings = np.zeros((seq_len, d_model))
        positions = np.arange(0, seq_len, dtype=np.float32)[:, np.newaxis]
        even_odd_i = np.arange(0, d_model, 2)
        div_freqs_term = np.power(10000, even_odd_i / np.float32(d_model))
        div_freqs_term = div_freqs_term[np.newaxis, :]
        div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))
        position_encodings[:, 0::2] = np.sin(positions * div_term)
        position_encodings[:, 1::2] = np.cos(positions * div_term)
        position_encodings = position_encodings[np.newaxis, :, :]
        self.position_encodings = tf.constant(position_encodings, dtype=tf.float32)
        self.dropout = Dropout(dropout_p)

    def call(self, x):
        x = x + self.position_encodings[:, :x.shape[1], :]
        pos_encoding = self.dropout(x)
        return pos_encoding

class FeedForward(tf.keras.layers.Layer):
    def __init__(self, d_model: int, ff_dropout: float) -> None:
        super().__init__()
        self.d_model = d_model
        self.ff_dropout = ff_dropout
        self.d_ff = self.d_model * 4
        self.linear_1 = Dense(units=self.d_ff)  # Linear layer 1
        self.linear_2 = Dense(units=self.d_model)  # Linear Layer 2
        self.ff_dropout_layer = Dropout(rate=self.ff_dropout)  # Dropout layer with rate = ff_dropout

    def call(self, inputs, training=None, mask=None):
        linear_1_out = self.linear_1(inputs)  # Linear layer 1 out
        linear_1_out = tf.nn.relu(linear_1_out)  # Applying ReLU activation function
        linear_1_out = self.ff_dropout_layer(linear_1_out, training=training)  # Applying Dropout
        ff_out = self.linear_2(linear_1_out)  # Linear layer 2 -> FF Block output
        return ff_out


class LayerNorm(tf.keras.layers.Layer):
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.alpha = tf.Variable(tf.ones(d_model), trainable=True)
        self.beta = tf.Variable(tf.zeros(d_model), trainable=True)

    def call(self, x):
        mean = tf.reduce_mean(x, axis=-1, keepdims=True)
        std = tf.math.reduce_std(x, axis=-1, keepdims=True)
        x_normalized = (x - mean) / (std + self.eps)
        layer_norm_out = self.alpha * x_normalized + self.beta
        return layer_norm_out

class ResidualConnection(tf.keras.layers.Layer):
    def __init__(self, d_model, res_dropout):
        super().__init__()
        self.dropout_layer = tf.keras.layers.Dropout(res_dropout)
        self.layer_norm_layer = LayerNorm(d_model)

    def call(self, x, sublayer):
        sublayer_out = self.dropout_layer(sublayer(self.layer_norm_layer(x)))
        add_layer_norm_out = x + sublayer_out
        return add_layer_norm_out

class EncoderBlock(tf.keras.layers.Layer):

    def __init__(self, d_model, num_heads, attn_dropout, ff_dropout, res_dropout):
        super(EncoderBlock, self).__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.attn_dropout = attn_dropout
        self.ff_dropout = ff_dropout
        self.res_dropout = res_dropout

        self.self_attention = MultiHeadAttention(self.d_model, self.num_heads, self.attn_dropout)
        self.feed_forward = FeedForward(self.d_model, self.ff_dropout)
        self.res_connection1 = ResidualConnection(self.d_model, self.res_dropout)
        self.res_connection2 = ResidualConnection(self.d_model, self.res_dropout)

    def call(self, x, src_attn_mask):
        attn_out = self.res_connection1(x, lambda x: self.self_attention(x, x, x, src_attn_mask))
        enc_block_out = self.res_connection2(attn_out, self.feed_forward)
        return enc_block_out

class Encoder(tf.keras.layers.Layer):

    def __init__(self, d_model, num_layers):
        super(Encoder, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        self.encoder_blocks = [EncoderBlock(d_model, NUM_HEADS, ATTN_DROPOUT, FF_DROPOUT, RES_DROPOUT) for _ in range(num_layers)]
        self.layer_norm = LayerNorm(d_model, EPS)

    def call(self, x, attn_mask):

        for layer in self.encoder_blocks:
            x = layer(x, attn_mask)
        encoder_out = self.layer_norm(x)
        return encoder_out

class DecoderBlock(tf.keras.layers.Layer):

    def __init__(self, d_model, num_heads, attn_dropout, ff_dropout, res_dropout):
        super(DecoderBlock, self).__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.attn_dropout = attn_dropout
        self.ff_dropout = ff_dropout
        self.res_dropout = res_dropout
        self.self_attention = MultiHeadAttention(self.d_model, self.num_heads, self.attn_dropout)
        self.cross_attention = MultiHeadAttention(self.d_model, self.num_heads, self.attn_dropout)
        self.feed_forward = FeedForward(self.d_model, self.ff_dropout)
        self.res_connection1 = ResidualConnection(self.d_model, self.res_dropout)
        self.res_connection2 = ResidualConnection(self.d_model, self.res_dropout)
        self.res_connection3 = ResidualConnection(self.d_model, self.res_dropout)

    def call(self, x, encoder_out, src_attn_mask, tgt_attn_mask):
        self_attn_out = self.res_connection1(x, lambda x: self.self_attention(x, x, x, tgt_attn_mask))
        cross_attn_out = self.res_connection2(self_attn_out, lambda self_attn_out: self.cross_attention(self_attn_out, encoder_out, encoder_out, src_attn_mask))
        dec_block_out = self.res_connection3(cross_attn_out, self.feed_forward)
        return dec_block_out

class Decoder(tf.keras.layers.Layer):
    def __init__(self, d_model, num_layers):
        super(Decoder, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        self.decoder_blocks = [DecoderBlock(d_model, NUM_HEADS, ATTN_DROPOUT, FF_DROPOUT, RES_DROPOUT) for _ in range(num_layers)]
        self.layer_norm = LayerNorm(d_model, EPS)

    def call(self, x, encoder_out, src_attn_mask, tgt_attn_mask):
        for layer in self.decoder_blocks:
            x = layer(x, encoder_out, src_attn_mask, tgt_attn_mask)
        decoder_out = self.layer_norm(x)
        return decoder_out

class Transformer(tf.keras.Model):
    def __init__(self, d_model, num_layers, vocab_size):
        super(Transformer, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        self.vocab_size = vocab_size
        self.inp_embedding = InputEmbeddings()                                  # Assuming InputEmbeddings is a custom layer or an embedding layer
        self.encoder = Encoder(self.d_model, self.num_layers)                   # Assuming Encoder is a custom layer
        self.decoder = Decoder(self.d_model, self.num_layers)                   # Assuming Decoder is a custom layer
        self.projection = tf.keras.layers.Dense(self.vocab_size)                # Projection layer

    def call(self, src_x, src_attn_mask, tgt_x, tgt_attn_mask):

        src_embed = self.inp_embedding(src_x)
        tgt_embed = self.inp_embedding(tgt_x)
        encoder_out = self.encoder(src_embed, src_attn_mask)
        decoder_out = self.decoder(tgt_embed, encoder_out, src_attn_mask, tgt_attn_mask)
        transformer_out = self.projection(decoder_out)
        return transformer_out

def build_transformer_model(d_model, num_layers, vocab_size):
    transformer = Transformer(d_model, num_layers, vocab_size)
    for layer in transformer.layers:
        if isinstance(layer, tf.keras.layers.Dense):
            layer.kernel_initializer = tf.keras.initializers.GlorotUniform()
    return transformer

import pickle
transformer = build_transformer_model(D_MODEL, NUM_LAYERS, VOCAB_SIZE)
class TransformerTokenizerTrainer:
    def __init__(self, data_id, tokenizer_path):
        self.data_id = data_id
        self.tokenizer_path = tokenizer_path
        self.vocab_size = VOCAB_SIZE

    def dataset_load(self):
        dataset = load_dataset(self.data_id, split="train")
        return dataset

    def dataset_iterator(self):
        dataset = self.dataset_load()
        for data in dataset:
            yield data["text"]

    def build_tokenizer(self):
        # if path.exists(self.tokenizer_path):
        special_tokens = ["[PAD]", "[SOS]", "[EOS]", "[UNK]"]
        tokenizer = Tokenizer(filters='!"#$%&()*+,-./:;=?@[\\]^_`{|}~\t\n',oov_token="<UNK>")
        tokenizer.fit_on_texts(self.dataset_iterator())
        for token in special_tokens:
          tokenizer.word_index[token] = len(tokenizer.word_index) + 1
        with open('/content/tokenizer.pickle', 'wb') as handle:
          pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
        return tokenizer

class ClassificationDataset(tf.keras.utils.Sequence):
    def __init__(self, dataset_id, tokenizer, max_seq_len, src_cln_name, tgt_cln_name):
        super().__init__()
        with open('/content/tokenizer.pickle', 'rb') as handle:
          tokenizer = pickle.load(handle)
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.src_cln_name = src_cln_name
        self.tgt_cln_name = tgt_cln_name
        self.dataset_id = dataset_id
        self.dataset = HFDataset.from_pandas(pd.read_parquet(dataset_id).sample(frac=1))
        self.sos_token = tf.convert_to_tensor([tokenizer.word_index["[SOS]"]], dtype=tf.int64)
        self.eos_token = tf.convert_to_tensor([tokenizer.word_index["[EOS]"]], dtype=tf.int64)
        self.pad_token = tf.convert_to_tensor([tokenizer.word_index["[PAD]"]], dtype=tf.int64)
    def __len__(self):
        return len(self.dataset)
    def __getitem__(self, index):
        try:
            # print("####################################")
            # print(index)
            data = self.dataset[index]
            src_data = data[self.src_cln_name]
            tgt_data = data[self.tgt_cln_name]
            #print(src_data)
            encoder_inp_tokens = self.tokenizer.texts_to_sequences(src_data)
            decoder_inp_tokens = self.tokenizer.texts_to_sequences(str(tgt_data))

            if encoder_inp_tokens.shape[0] > self.max_seq_len-2:
                encoder_inp_tokens = encoder_inp_tokens[:self.max_seq_len-2]

            encoder_num_padding_tokens = self.max_seq_len - encoder_inp_tokens.shape[0] - 2
            decoder_num_padding_tokens = self.max_seq_len - decoder_inp_tokens.shape[0] - 1

            encoder_inp_pad_tokens = tf.convert_to_tensor([self.pad_token] * encoder_num_padding_tokens, dtype=tf.int64)
            decoder_inp_pad_tokens = tf.convert_to_tensor([self.pad_token] * decoder_num_padding_tokens, dtype=tf.int64)
            label_pad_tokens = tf.convert_to_tensor([self.pad_token] * decoder_num_padding_tokens, dtype=tf.int64)

            encoder_inp_tokens = tf.expand_dims(encoder_inp_tokens, axis=0)
            encoder_inp_tokens = tf.concat([tf.expand_dims(self.sos_token, axis=0), encoder_inp_tokens,
                tf.expand_dims(self.eos_token, axis=0), tf.reshape(encoder_inp_pad_tokens, (1, -1))], axis=1)
            decoder_inp_tokens = tf.concat([tf.expand_dims(self.sos_token, axis=0),
                tf.expand_dims(decoder_inp_tokens, axis=0), tf.reshape(decoder_inp_pad_tokens, (1, -1))], axis=1)
            label_tokens = tf.concat([decoder_inp_tokens,tf.expand_dims(self.sos_token, axis=0), tf.reshape(label_pad_tokens, (1, -1))], axis=1)

            encoder_attn_mask = tf.expand_dims(tf.cast(encoder_inp_tokens != self.pad_token, dtype=tf.int64), axis=0)
            auto_regressive_mask = tf.cast(tf.linalg.band_part(tf.ones((tf.size(decoder_inp_tokens), tf.size(decoder_inp_tokens))), -1, 0), dtype=tf.int64)
            decoder_attn_mask = tf.cast(decoder_inp_tokens != self.pad_token, dtype=tf.int64) #& (tf.cast(auto_regressive_mask, dtype=tf.int64) == 0)

            model_inp = {
                "encoder_input_ids": encoder_inp_tokens,
                "decoder_input_ids": decoder_inp_tokens,
                "encoder_attention_mask": encoder_attn_mask,
                "decoder_attention_mask": decoder_attn_mask,
                "labels": label_tokens,
                "source": src_data,
                "target": tgt_data
            }
            return model_inp
        except Exception as e:
            print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")
            #print(data = self.dataset[index])
            print("Error:",e,data)
            print(traceback.print_exc(limit=None))

import tensorflow as tf
import pandas as pd
#from transformers import HFDataset
import pickle
import traceback

class ClassificationDataset(tf.keras.utils.Sequence):
    def __init__(self, dataset_id, tokenizer, max_seq_len, src_cln_name, tgt_cln_name):
        super().__init__()
        with open('/content/tokenizer.pickle', 'rb') as handle:
            self.tokenizer = pickle.load(handle)
        self.max_seq_len = max_seq_len
        self.src_cln_name = src_cln_name
        self.tgt_cln_name = tgt_cln_name
        self.dataset_id = dataset_id
        self.dataset = HFDataset.from_pandas(pd.read_parquet(dataset_id).sample(frac=1))
        self.sos_token = tf.convert_to_tensor([self.tokenizer.word_index["[SOS]"]], dtype=tf.int64)
        self.eos_token = tf.convert_to_tensor([self.tokenizer.word_index["[EOS]"]], dtype=tf.int64)
        self.pad_token = tf.convert_to_tensor([self.tokenizer.word_index["[PAD]"]], dtype=tf.int64)

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index):
        data = self.dataset[index]
        src_data = data[self.src_cln_name]
        tgt_data = data[self.tgt_cln_name]

        encoder_inp_tokens = self.tokenizer.texts_to_sequences([src_data])[0]
        decoder_inp_tokens = self.tokenizer.texts_to_sequences([str(tgt_data)])[0]

        if len(encoder_inp_tokens) > self.max_seq_len - 2:
            encoder_inp_tokens = encoder_inp_tokens[:self.max_seq_len - 2]

        if len(decoder_inp_tokens) > self.max_seq_len - 1:
            decoder_inp_tokens = decoder_inp_tokens[:self.max_seq_len - 1]

        encoder_num_padding_tokens = self.max_seq_len - len(encoder_inp_tokens) - 2
        decoder_num_padding_tokens = self.max_seq_len - len(decoder_inp_tokens) - 1

        encoder_inp_tokens = [self.sos_token] + encoder_inp_tokens + [self.eos_token] + [self.pad_token] * encoder_num_padding_tokens
        decoder_inp_tokens = [self.sos_token] + decoder_inp_tokens + [self.pad_token] * decoder_num_padding_tokens
        label_tokens = decoder_inp_tokens + [self.sos_token] + [self.pad_token] * decoder_num_padding_tokens

        # Convert lists to tensors
        encoder_inp_tokens = tf.convert_to_tensor(encoder_inp_tokens, dtype=tf.int64)
        decoder_inp_tokens = tf.convert_to_tensor(decoder_inp_tokens, dtype=tf.int64)
        label_tokens = tf.convert_to_tensor(label_tokens, dtype=tf.int64)

        # Create attention masks
        encoder_attn_mask = tf.cast(tf.not_equal(encoder_inp_tokens, self.pad_token), dtype=tf.int64)[tf.newaxis, tf.newaxis, :]
        auto_regressive_mask = tf.cast(tf.linalg.band_part(tf.ones((1, tf.shape(decoder_inp_tokens)[0], tf.shape(decoder_inp_tokens)[0])), -1, 0), dtype=tf.int64)
        decoder_attn_mask = tf.cast(tf.not_equal(decoder_inp_tokens, self.pad_token), dtype=tf.int64)[tf.newaxis, :] & (auto_regressive_mask == 0)
        print(decoder_attn_mask)
        model_inp = {
            "encoder_input_ids": encoder_inp_tokens,
            "decoder_input_ids": decoder_inp_tokens,
            "encoder_attention_mask": encoder_attn_mask,
            "decoder_attention_mask": decoder_attn_mask,
            "labels": label_tokens,
            "source": src_data,
            "target": tgt_data
        }
        return model_inp

class TrainerUtils:
    def __init__(self):
        self.dataset_id = DATASET_ID
        self.tokenizer_path = TOKENIZER_PATH
        self.max_seq_len = MAX_SEQ_LEN
        self.src_cln_name = SRC_CLN_NAME
        self.tgt_cln_name = TGT_CLN_NAME
        self.logging_dir = LOG_DIR
        self.batch_size = BATCH_SIZE
        self.vocab_size = VOCAB_SIZE
        with open('/content/tokenizer.pickle', 'rb') as handle:
          tokenizer = pickle.load(handle)
        self.tokenizer = tokenizer
        #self.tokenizer = Tokenizer.from_file(self.tokenizer_path)
        self.transformer_model = build_transformer_model(d_model=D_MODEL, num_layers=NUM_LAYERS, vocab_size=self.vocab_size)
        self.train_classification_dataset = ClassificationDataset(self.dataset_id, self.tokenizer, self.max_seq_len, self.src_cln_name, self.tgt_cln_name)
        self.writer = TensorBoard(log_dir=self.logging_dir)
        self.optimizer = Adam(lr=LR)
        self.loss_fn = SparseCategoricalCrossentropy()                          # ignore_index=self.tokenizer.token_to_id('[PAD]')

    def tf_collate(self,batch):
        if isinstance(batch[0], tf.Tensor):
            batch = [tensor.numpy() for tensor in batch]
            return np.array(batch)
        else:
            return batch

    def load_dataloaders(self):
        train_dataloader = DataLoader(self.train_classification_dataset, batch_size=self.batch_size, collate_fn=self.tf_collate)
        return train_dataloader, train_dataloader

    def compute_loss(self, logits, labels):

        logits = tf.reshape(logits, (-1, self.vocab_size))
        loss = self.loss_fn(labels, logits)
        return loss

    def generate(self, model, source, src_attn_mask, tokenizer: Tokenizer):
        i = 0
        device = "cpu"
        sos_idx = tokenizer.token_to_id("[SOS]")
        eos_idx = tokenizer.token_to_id("[EOS]")
        decoder_inp = tf.fill((1, 1), sos_idx)
        while True:
            if decoder_inp.shape[1] == self.max_seq_len:
                break
            tgt_attn_mask = tf.linalg.band_part(tf.ones((1, decoder_inp.shape[1], decoder_inp.shape[1])), -1, 0)
            out = model(source, src_attn_mask, decoder_inp, tgt_attn_mask)
            logits = tf.nn.softmax(out, axis=2)
            next_word_logit_argmax = tf.argmax(logits, axis=2)
            next_word_logit_argmax = next_word_logit_argmax[0][-1]
            #print(tokenizer.decode_batch(decoder_inp.numpy().tolist()))
            decoder_inp = tf.concat([decoder_inp, tf.fill((1, 1), next_word_logit_argmax)], axis=1)
            i = i+1
            if next_word_logit_argmax == eos_idx:
                break

        model_out = tokenizer.decode_batch(decoder_inp.numpy().tolist())[0]
        return model_out

    def load_train_utils(self):
        train_dataloader, test_dataloader = self.load_dataloaders()
        return self.transformer_model, train_dataloader, test_dataloader, self.writer, self.optimizer

    def train_one_step(self, model_inp, model, optimizer):
      try:
        model_inp = model_inp[0]
        src_x = model_inp["encoder_input_ids"]
        src_attn_mask = model_inp["encoder_attention_mask"]
        tgt_x = model_inp["decoder_input_ids"]
        tgt_attn_mask = model_inp["decoder_attention_mask"]
        labels = model_inp["labels"]
        with tf.GradientTape() as tape:                                         # Example usage of sparse_softmax_cross_entropy_with_logits
            logits = tf.random.normal([32, 50000])                              # Example logits tensor with shape (batch_size, num_classes)
            labels = tf.random.uniform([32], maxval=50000, dtype=tf.int32)      # Example labels tensor with shape (batch_size,)
            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)
            model_logits = model(src_x, src_attn_mask, tgt_x, tgt_attn_mask)
            loss = self.compute_loss(model_logits, labels)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        # print("************** Train One Step Loss,Model,Optimizer *******************")
        # print(loss, model, optimizer)
        return loss, model, optimizer
      except:
        # print("************** Train One Step *******************")
        loss = 0
        model = "<__main__.Transformer object at 0x7ae00071a710>"
        optimizer = "<keras.src.optimizers.adam.Adam object at 0x7adff73dc6d0>"
        return loss, model, optimizer

    def ensure_save_folder(self, path):
        if not os.path.exists(path):
            os.makedirs(path)

    def log(self, writer, desc, scalar, current_step):
        tf.summary.scalar(desc, scalar, step=current_step)

    def save_checkpoint(self, model, optimizer, epoch_num, loss, model_save_dir_path):
        self.ensure_save_folder(path=model_save_dir_path)
        ckpt_path = os.path.join(model_save_dir_path, f"epoch-{epoch_num}-checkpoint.pth")
        print("@@@@@@@@@@@@@@@@@@@@@@",ckpt_path)
        model.save_weights(ckpt_path)
        return "Model Checkpoint saved successfully"


class Trainer:

    def __init__(self):
        self.trainer_utils = TrainerUtils()
        self.model, self.train_dataloader, self.test_dataloader, self.writer, self.optimizer = self.trainer_utils.load_train_utils()
        self.epoch_num = 0
        self.step_num = 0
        self.num_epochs = NUM_EPOCHS
        self.model_save_path = os.path.join(MODEL_SAVE_PATH)

    def train(self):
        for epoch in range(self.num_epochs):
            train_loss = []
            for data in tqdm(self.train_dataloader):
                loss, self.model, self.optimizer = self.trainer_utils.train_one_step(data, self.model, self.optimizer)
                # train_loss.append(loss)
                self.step_num += 1
                self.trainer_utils.log(self.writer, "train_loss", loss, self.step_num)
            self.epoch_num += 1
            saved_message = self.trainer_utils.save_checkpoint(self.model, self.optimizer, self.epoch_num, loss, self.model_save_path)
            print(saved_message)
            print(loss)

# from train_utils import TrainerUtils
from tqdm import tqdm
from torch.utils.data import DataLoader
transformer_tokenizer_trainer = TransformerTokenizerTrainer("imdb", "/content/tokenizer.json")
tokenizer = transformer_tokenizer_trainer.build_tokenizer()
trainer = Trainer()
trainer.train()

